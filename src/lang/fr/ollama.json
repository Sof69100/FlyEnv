{
  "OLLAMA_DEBUG": "Afficher des informations de débogage supplémentaires (par exemple OLLAMA_DEBUG=1)",
  "OLLAMA_HOST": "Adresse IP pour le serveur Ollama (par défaut 127.0.0.1:11434)",
  "OLLAMA_KEEP_ALIVE": "Durée pendant laquelle les modèles restent chargés en mémoire (par défaut \"5m\")",
  "OLLAMA_MAX_LOADED_MODELS": "Nombre maximum de modèles chargés par GPU",
  "OLLAMA_MAX_QUEUE": "Nombre maximum de requêtes en file d'attente",
  "OLLAMA_MODELS": "Chemin vers le répertoire des modèles",
  "OLLAMA_NUM_PARALLEL": "Nombre maximum de requêtes parallèles",
  "OLLAMA_NOPRUNE": "Ne pas purger les blobs de modèles au démarrage",
  "OLLAMA_ORIGINS": "Liste séparée par des virgules des origines autorisées",
  "OLLAMA_SCHED_SPREAD": "Toujours planifier les modèles sur tous les GPU",
  "OLLAMA_FLASH_ATTENTION": "Activer l'attention flash",
  "OLLAMA_KV_CACHE_TYPE": "Type de quantification pour le cache K/V (par défaut : f16)",
  "OLLAMA_LLM_LIBRARY": "Définir la bibliothèque LLM pour contourner la détection automatique",
  "OLLAMA_GPU_OVERHEAD": "Réserver une portion de VRAM par GPU (en octets)",
  "OLLAMA_LOAD_TIMEOUT": "Durée maximale pour permettre aux chargements de modèles de stagner avant d'abandonner (par défaut \"5m\")",
  "needServiceRun": "Veuillez d'abord démarrer le service Ollama",
  "size": "Taille",
  "model": "Modèles"
}